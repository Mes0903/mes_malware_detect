/**
 * @file logistic.cpp
 * @author Mes (mes900903@gmail.com) (Discord: Mes#0903)
 * @brief The implementation of the weak learner class in Adaboost, I use the logistic as the weak learner.
 * @version 0.1
 * @date 2022-11-17
 */

#include "feature_num.h"
#include "logistic.h"
#include "Eigen/Eigen"

#include <vector>
#include <cmath>
#include <filesystem>
#include <iostream>
#include <algorithm>
#include <tuple>
#include <random>
#include <fstream>
#include <sstream>


/**
 * @brief Training the weight in weak learner
 *
 * @param train_X The training data, which is a feature matrix.
 * @param train_Y The training label.
 * @param train_weight The training weight in adaboost.
 * @param Iterations The training iterations.
 * @return std::tuple<Eigen::VectorXd, double, bool> The first element of the pair is the label it predict,
 *                                            the second one is the error rate,
 *                                            the third one is a flag for 100% accuracy, if the accuracy is 100%, we can delete all the other weak learner in adaboost.
 */
std::tuple<Eigen::MatrixXd, double, bool>
logistic::fit(const Eigen::MatrixXd &train_X, const Eigen::MatrixXd &train_Y, const Eigen::VectorXd &train_weight, uint32_t Iterations)
{
  uint32_t D = FEATURE_NUM;    // dimention is the column of training data, which is 5 in my case, since there is 5 features.
  uint32_t d = CLASS_NUM;

  // use random initialize weight generated by normal distribution
  static std::random_device rd;
  static std::default_random_engine gen(rd());
  std::normal_distribution<> dis(0, std::sqrt(D + 1));
  w = Eigen::MatrixXd::NullaryExpr(D, d, [&]() { return dis(gen); });
  w0 = Eigen::RowVectorXd::NullaryExpr(d, [&]() { return dis(gen); });

  bool all_correct = false;
  double mse = 0;

  for (uint32_t i = 0; i < Iterations; ++i) {
    Eigen::ArrayXXd hx = (train_X * w).rowwise() + w0;
    hx = cal_softmax(hx);
    Eigen::MatrixXd tmp = train_Y.array() - hx;
    tmp.array().colwise() *= train_weight.array();
    // std::cout << tmp.cols() << '\n';

    mse = tmp.array().square().sum();
    // std::cout << mse << '\n';
    if (mse < 0.0001)
      all_correct = true;

    Eigen::MatrixXd grad_w = train_X.transpose() * tmp;
    Eigen::RowVectorXd grad_w0 = tmp.colwise().sum();
    w += 0.001 * grad_w;
    w0 += 0.001 * grad_w0;
  }

  Eigen::MatrixXd pred_Y = predict(train_X);

  return { pred_Y, mse, all_correct };
}

/**
 * @brief Calculate the logistic function 1 / (1 + e^(-x)), which is equivalent to (tanh(x / 2) + 1)/2.
 *
 * @param x The input array.
 * @return Eigen::ArrayXd
 */
Eigen::ArrayXXd logistic::cal_softmax(const Eigen::ArrayXXd &x) const
{
  Eigen::MatrixXd tmp = x.exp();
  return tmp.array().rowwise().sum().inverse().matrix().asDiagonal() * tmp;
}

/**
 * @brief Make prediction of the data, this function is for debug using, it's output is not an label but an probability.
 *
 * @param data The feature matrix of all section, the size is Sn*5, Sn is the total number of the data, 5 means the number of the feature.
 * @return Eigen::VectorXd The probability of the data get from the logistic function.
 */
Eigen::MatrixXd logistic::predict(const Eigen::MatrixXd &data) const
{
  Eigen::MatrixXd hx = (data * w).rowwise() + w0;

  return cal_softmax(hx);
}

/**
 * @brief Store the weight of the weak learner.
 *
 * @param outfile The file path, where to store the weight.
 */
void logistic::store_weight(std::ofstream &outfile) const
{
  for (int i = 0; i < CLASS_NUM; ++i)
    outfile << w0(i) << ' ';

  outfile << '\n';

  for (int i = 0; i < FEATURE_NUM; ++i) {
    for (int j = 0; j < CLASS_NUM; ++j)
      outfile << w(i, j) << ' ';

    outfile << '\n';
  }

  outfile << '\n';
}

/**
 * @brief Load the weight of the weak learner.
 *
 * @param infile The file path, where to load the weight.
 * @param stream For avoiding copying std::stringstream, pass it by reference into function.
 */
void logistic::load_weight(std::ifstream &infile)
{
  uint32_t D = FEATURE_NUM;    // dimention is the column of training data, which is 5 in my case, since there is 5 features.
  uint32_t d = CLASS_NUM;
  w = Eigen::MatrixXd::Zero(D, d);
  w0 = Eigen::RowVectorXd::Zero(d);

  std::string line;
  std::stringstream stream;

  getline(infile, line);
  stream << line;
  for (int i = 0; i < CLASS_NUM; ++i)
    stream >> w0(i);

  stream.str("");
  stream.clear();

  for (int i = 0; i < FEATURE_NUM; ++i) {
    getline(infile, line);
    stream << line;
    for (int j = 0; j < CLASS_NUM; ++j) {
      stream >> w(i, j);
    }
  }
}