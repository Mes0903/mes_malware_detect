/**
 * @file logistic.cpp
 * @author Mes (mes900903@gmail.com) (Discord: Mes#0903)
 * @brief The implementation of the weak learner class in Adaboost, I use the logistic as the weak learner.
 * @version 0.1
 * @date 2022-11-17
 */

#include "file_handler.h"
#include "feature_num.h"
#include "logistic.h"
#include "Eigen/Eigen"

#include <vector>
#include <cmath>
#include <filesystem>
#include <iostream>
#include <algorithm>
#include <tuple>
#include <random>
#include <fstream>
#include <sstream>

/**
 * @brief Training the weight in weak learner
 *
 * @param train_X The training data, which is a feature matrix.
 * @param train_Y The training label.
 * @param train_weight The training weight in adaboost.
 * @param Iterations The training iterations.
 * @return std::tuple<Eigen::VectorXd, double, bool> The first element of the pair is the label it predict,
 *                                            the second one is the error rate,
 *                                            the third one is a flag for 100% accuracy, if the accuracy is 100%, we can delete all the other weak learner in adaboost.
 */
void logistic::fit(const Eigen::MatrixXd &train_X, const Eigen::MatrixXd &train_Y, const Eigen::VectorXd &train_weight, uint32_t Iterations)
{
  uint32_t D = FEATURE_NUM;    // dimention is the column of training data, which is 5 in my case, since there is 5 features.
  uint32_t d = CLASS_NUM;

  // use random initialize weight generated by normal distribution
  static std::random_device rd;
  static std::default_random_engine gen(rd());
  std::normal_distribution<> dis(0, std::sqrt(D + 1));
  w = Eigen::MatrixXd::NullaryExpr(D, d, [&]() { return dis(gen); });
  w0 = Eigen::RowVectorXd::NullaryExpr(d, [&]() { return dis(gen); });

  Eigen::MatrixXd w_momentum = Eigen::MatrixXd::Zero(D, d);
  Eigen::RowVectorXd w0_momentum = Eigen::RowVectorXd::Zero(d);
  double alpha = 0.1;
  for (uint32_t i = 0; i < Iterations; ++i) {
    double mse = 0;
    double lr = 5;
    Eigen::ArrayXXd hx = (train_X * w).rowwise() + w0;
    hx = cal_softmax(hx);
    Eigen::MatrixXd tmp = train_weight.asDiagonal() * (train_Y.array() - hx).matrix();
    mse = (tmp.array().square().colwise() * train_weight.array()).sum();
    Eigen::MatrixXd w_grad = train_X.transpose() * tmp;
    Eigen::RowVectorXd w0_grad = tmp.colwise().sum();

    std::cout << "\rmse: " << train_weight << std::flush;
    w_momentum = (w_momentum + lr * w_grad) * 0.9;
    w0_momentum = (w0_momentum + lr * w0_grad) * 0.9;
    w += w_momentum + lr * w_grad;
    w0 += w0_momentum + lr * w0_grad;
  }
}

/**
 * @brief Calculate the logistic function 1 / (1 + e^(-x)), which is equivalent to (tanh(x / 2) + 1)/2.
 *
 * @param x The input array.
 * @return Eigen::ArrayXd
 */
Eigen::ArrayXXd logistic::cal_softmax(const Eigen::ArrayXXd &x) const
{
  Eigen::MatrixXd tmp = x.exp();
  return tmp.array().rowwise().sum().inverse().matrix().asDiagonal() * tmp;
}

/**
 * @brief Make prediction of the data, this function is for debug using, it's output is not an label but an probability.
 *
 * @param data The feature matrix of all section, the size is Sn*5, Sn is the total number of the data, 5 means the number of the feature.
 * @return Eigen::VectorXd The probability of the data get from the logistic function.
 */
Eigen::MatrixXd logistic::predict(const Eigen::MatrixXd &data) const
{
  Eigen::MatrixXd hx = (data * w).rowwise() + w0;

  return cal_softmax(hx);
}

Eigen::MatrixXd logistic::get_label(const Eigen::MatrixXd &x) const
{
  Eigen::MatrixXd pred = predict(x);
  Eigen::MatrixXd output = Eigen::MatrixXd::Zero(pred.rows(), pred.cols());
  for (uint32_t i = 0; i < pred.rows(); ++i) {
    Eigen::VectorXd::Index maxIndex;
    pred.row(i).maxCoeff(&maxIndex);
    output(i, maxIndex) = 1;
  }

  return output;
}

/**
 * @brief Store the weight of the weak learner.
 *
 * @param outfile The file path, where to store the weight.
 */
void logistic::store_weight(std::ofstream &outfile) const
{
  for (int i = 0; i < CLASS_NUM; ++i)
    outfile << w0(i) << ' ';

  outfile << '\n';

  for (int i = 0; i < FEATURE_NUM; ++i) {
    for (int j = 0; j < CLASS_NUM; ++j)
      outfile << w(i, j) << ' ';

    outfile << '\n';
  }

  outfile << '\n';
}

/**
 * @brief Load the weight of the weak learner.
 *
 * @param infile The file path, where to load the weight.
 * @param ss For avoiding copying std::stringstream, pass it by reference into function.
 */
void logistic::load_weight(std::ifstream &infile)
{
  uint32_t D = FEATURE_NUM;    // dimention is the column of training data, which is 5 in my case, since there is 5 features.
  uint32_t d = CLASS_NUM;

  w = Eigen::MatrixXd::Zero(D, d);
  w0 = Eigen::RowVectorXd::Zero(d);

  std::string line;
  std::stringstream ss;

  getline(infile, line);
  ss << line;
  for (int i = 0; i < CLASS_NUM; ++i)
    ss >> w0(i);

  ss.str("");
  ss.clear();

  for (int i = 0; i < FEATURE_NUM; ++i) {
    getline(infile, line);
    ss << line;
    for (int j = 0; j < CLASS_NUM; ++j) {
      ss >> w(i, j);
    }
  }
}